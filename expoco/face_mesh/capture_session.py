# AUTOGENERATED! DO NOT EDIT! File to edit: 10b_mediapipe_face_mesh_capture_session.ipynb (unless otherwise specified).

__all__ = ['mp_face_mesh', 'COLUMN_NAMES', 'dry_run', 'capture_session']

# Cell
from ..core import *
import ipywidgets as widgets # TODO: remove if not used
import numpy as np
import pandas as pd
import cv2, time, math, json, shutil
import win32api, win32con

import mediapipe as mp
mp_face_mesh = mp.solutions.face_mesh

from pathlib import Path

# Cell
COLUMN_NAMES = landmark_ids_to_col_names(range(468), None, ['x','y','z'])

# Cell
def _new_metadata(stop_after, path, video_capture, expression_id, expression_name, comments):
    width, height = [int(video_capture.get(p)) for p in [cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT]]
    assert width >= height
    return dict(count=0, stop_after=stop_after, path=str(path.resolve()), expression_id=expression_id,
                expression_name=expression_name, capture_width=width, capture_height=height, start_date=now(),
                column_names=COLUMN_NAMES, comments=comments)
# TODO: add relevant software versions etc

# Cell
def _setup_variables(expression_id):
    expression_id = str(expression_id)
    with open('data/viseme-config.json') as f: config = json.load(f)
    if expression_id not in config.get('expressions', {}):
        raise Exception(f'{expression_id} is missing from expressions section of data/viseme-config.json')
    expression_name = config['expressions'][expression_id]
    path = Path(f'data/capture_session/viseme_{now()}_{expression_id}') # TODO: capture_sessions
    path.mkdir(parents=True, exist_ok=True)
    video_capture = cv2.VideoCapture(0)
    width, height = [int(video_capture.get(p)) for p in [cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT]]
    assert width >= height
    return expression_id, expression_name, path, video_capture, width, height

# Cell
def _update_image(image, image_display_helper, text):
    image = cv2.putText(image, text, (20,40), cv2.FONT_HERSHEY_COMPLEX, 1, (200,200,200))
    image_display_helper.show(image)

# Cell
def _capture_and_process(video_capture, face_mesh): # TODO: DRY
    retval, image = video_capture.read() # TODO: check retval
    image = cv2.flip(image, 1)
    return image, face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

# Cell
def _countdown(video_capture, expression_name, image_display_helper, face_mesh):
    for i in range(3,0,-1):
        image, _ = _capture_and_process(video_capture, face_mesh)
        _update_image(image, image_display_helper, f'Capture: {expression_name} in {i}s')
        time.sleep(1)

# Cell
def _save_results(path, results, data, image, capture_count):
    # save all landmarks calculated
    row = []
    for landmark_id in range(468):
        landmark = results.multi_face_landmarks[0].landmark[landmark_id]
        for coord in ['x','y','z']:
            row.append(getattr(landmark, coord))
    data.append(row)
    # save one in 10 images
#     if capture_count % 10 == 0:
#         img_name = f'{now()}_{capture_count}.png'
#         assert cv2.imwrite(f'{path}/{img_name}', image)
#         data['img_path'].append(img_name)
#     else:
#         data['img_path'].append('')
    # but we can save all as low res - using less space than a single png (even at max compression)
    assert cv2.imwrite(f'{path}/{capture_count}.jpeg', image, [cv2.IMWRITE_JPEG_QUALITY, 50])

# Cell
def dry_run():
    video_capture = cv2.VideoCapture(0)
    retval, image = video_capture.read()
    image_display_helper = ImageDisplayHelper(cv2.flip(image, 1), 'expoco: Dry Run')
    while True:
        if win32api.GetAsyncKeyState(win32con.VK_ESCAPE):
            video_capture.release()
            break
        retval, image = video_capture.read()
        image_display_helper.show(cv2.flip(image, 1))
        time.sleep(.05)
    image_display_helper.close()
    return image

# Cell
def capture_session(expression_id, stop_after, comments):
    "Run a video capture session"
    expression_id, expression_name, path, video_capture, width, height = _setup_variables(expression_id)
    data = []
    face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1)
    image, _ = _capture_and_process(video_capture, face_mesh)
    image_display_helper = ImageDisplayHelper(image, 'expoco: Capture session')
    try:
        _countdown(video_capture, expression_name, image_display_helper, face_mesh)
        metadata = _new_metadata(
                stop_after, path, video_capture, expression_id, expression_name, comments)
        for capture_count in range(1, stop_after+1):
            image, results = _capture_and_process(video_capture, face_mesh)
            _update_image(image, image_display_helper, f'{expression_name} {capture_count}')
            _save_results(path, results, data, image, capture_count)
            time.sleep(.05)
        metadata['count'] = capture_count
        metadata['end_date'] = now()
        with open(path/'metadata.json', 'w') as f: json.dump(metadata, f, indent=2)
        np.save(path/'data.npy', np.array(data, dtype=float), allow_pickle=False)
    finally:
        video_capture.release()
    return path