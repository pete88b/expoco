# AUTOGENERATED! DO NOT EDIT! File to edit: 05a_ml_data.ipynb (unless otherwise specified).

__all__ = ['viseme_dataset_from_capture_sessions', 'get_image_path', 'read_viseme_dataset',
           'select_viseme_dataset_columns', 'make_landmarks_relative', 'change_target', 'calculate_stats', 'normalize',
           'inference_data_from_landmarks', 'processed_dataset_from_viseme_dataset', 'read_processed_dataset']

# Cell
from ..core import *
from pathlib import Path
import numpy as np
import pandas as pd
import json

# Cell
def viseme_dataset_from_capture_sessions(input_path='data/capture_session', glob_pattern='viseme*'):
    "Create a viseme dataset from capture session data"
    input_path, dataset_id, data = Path(input_path), now(), []
    output_path = input_path.parent/f'viseme_dataset_{dataset_id}'
    output_path.mkdir()
    metadata = dict(input_path=path_to_str(input_path), output_path=path_to_str(output_path),
                    glob_pattern=glob_pattern, session_metadata=[], start_date=now())
    for session_path in sorted(input_path.glob(glob_pattern)):
        with open(session_path/'metadata.json') as f:
            session_metadata = json.load(f)
        column_names, expression_id = session_metadata['column_names'], session_metadata['expression_id']
        metadata['column_names'] = column_names # TODO: check that columns names match in all sessions
        del session_metadata['column_names']
        metadata['session_metadata'].append(session_metadata)
        _data = np.load(session_path/'data.npy')
        _expression_id_data = np.array([expression_id]*_data.shape[0], dtype=float)[..., None]
        _data = np.concatenate([_data, _expression_id_data], axis=1)
        data.append(_data)
    metadata['column_names'] += ['expression_id']
    with open(output_path/'metadata.json', 'w') as f: json.dump(metadata, f, indent=2)
    np.save(output_path/'data.npy', np.concatenate(data), allow_pickle=False)
    return output_path

# Cell
def get_image_path(dataset_path, row_idx):
    dataset_path = Path(dataset_path)
    if dataset_path.name.startswith('processed'):
        # TODO: read processed metadata and find dataset path - don't just assume its the parent
        dataset_path = dataset_path.parent
    with open(dataset_path/'metadata.json') as f:
        dataset_metadata = json.load(f)
    idx = row_idx
    for session_metadata in dataset_metadata['session_metadata']:
        if idx < session_metadata['count']:
            return Path(session_metadata['path'])/f'{idx+1}.jpeg'
        idx -= session_metadata['count']

# Cell
def read_viseme_dataset(dataset_path, to_numpy=True):
    "Return metadata and data (as a numpy array by default, pandas dataframe if `to_numpy=False`)"
    dataset_path = Path(dataset_path)
    data = np.load(dataset_path/'data.npy')
    with open(dataset_path/'metadata.json') as f:
        dataset_metadata = json.load(f)
    if to_numpy:
        return dataset_metadata, data
    return dataset_metadata, pd.DataFrame(data, columns=dataset_metadata['column_names'])

# Cell
def select_viseme_dataset_columns(dataset_metadata, data, column_names):
    return data[..., [c in column_names for c in dataset_metadata['column_names']]]

# Cell
def make_landmarks_relative(data, to_landmarks, column_count=None):
    "Make all landmarks in `data` relative `to_landmarks`"
    coord_count = to_landmarks.shape[-1]
    assert coord_count == 2, f'to_landmarks must have 2 "coord_count". to_arr.shape={coord_count}'
    for i in range(coord_count):
        data[:, i:column_count:coord_count] -= to_landmarks[..., i][..., None]
    return data

# Cell
def change_target(data, from_id, to_id):
    "Change values in the last column of `data` from one ID to another"
    data[data[..., -1] == from_id] = to_id
    return data

# Cell
def calculate_stats(data, column_count=None):
    "Return the mean and standard deviation over columns of `data`"
    if column_count is None:
        column_count = data.shape[-1]
    _d = data[..., :column_count]
    return dict(mean=_d.mean(axis=0), std=_d.std(axis=0))

# Cell
def normalize(data, mean, std):# TODO: xxx do not touch target columns...
    column_count = len(mean) # use len rather than shape[0] so that mean stats can be lists or np arrays
    data[..., :column_count] -= mean
    data[..., :column_count] /= std
    return data

# Cell
def inference_data_from_landmarks(landmarks, landmark_ids, relative_landmark_id=None, coords=['x', 'y'], stats=None):
    landmark_ids, coords, data, to_landmarks_data = sorted(landmark_ids), sorted(coords), [], []
    for landmark_id in landmark_ids:
        for coord in coords:
            if landmark_id == relative_landmark_id:
                to_landmarks_data.append(getattr(landmarks[landmark_id], coord))
            else:
                data.append(getattr(landmarks[landmark_id], coord))
    data, to_landmarks_data = np.array([data], dtype=float), np.array([to_landmarks_data], dtype=float)
    if relative_landmark_id is not None:
        make_landmarks_relative(data, to_landmarks_data)
    if stats is not None:
        normalize(data, **stats)
    return data

# Cell
def processed_dataset_from_viseme_dataset(
        input_path, landmark_ids, relative_landmark_id=None, y_name='expression_id',
        change_y_from=None, change_y_to=None): # TODO: named tuple for these 2?
    "Create a processed dataset (ready for ML) from a raw viseme dataset"
    input_path = Path(input_path)
    input_metadata, input_data = read_viseme_dataset(input_path)
    output_path = Path(input_metadata['output_path'])/f'processed_{now()}'
    output_path.mkdir()
    cont_names = landmark_ids_to_col_names(landmark_ids, relative_landmark_id)
    dataset_columns = cont_names + [y_name]
    metadata = dict(landmark_ids=landmark_ids,
                    relative_landmark_id=relative_landmark_id,
                    input_path=path_to_str(input_path),
                    output_path=path_to_str(output_path),
                    y_name=y_name,
                    change_y_from=change_y_from,
                    change_y_to=change_y_to,
                    cont_names=cont_names,
                    dataset_columns=dataset_columns)
    data = select_viseme_dataset_columns(input_metadata, input_data, dataset_columns)
    if change_y_from is not None:
        assert change_y_to is not None
        data = change_target(data, change_y_from, change_y_to)
    if relative_landmark_id is not None:
        relative_names = landmark_ids_to_col_names([relative_landmark_id])
        metadata['relative_names']=relative_names
        to_landmarks = select_viseme_dataset_columns(input_metadata, input_data, relative_names)
        make_landmarks_relative(data, to_landmarks, column_count=len(cont_names))
    stats = calculate_stats(data, column_count=len(cont_names))
    normalize(data, **stats)
    with open(output_path/'metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    np.save(output_path/'data.npy', data, allow_pickle=False)
    np.savez(output_path/'stats.npz', **stats)
    return output_path

# Cell
def read_processed_dataset(dataset_path, to_numpy=True):
    "Return metadata, data and stats (as a numpy array by default, pandas dataframe if `to_numpy=False`)"
    dataset_path = Path(dataset_path)
    data, stats = np.load(dataset_path/'data.npy'), np.load(dataset_path/'stats.npz')
    with open(dataset_path/'metadata.json') as f:
        dataset_metadata = json.load(f)
    if to_numpy:
        return dataset_metadata, data, stats
    columns = dataset_metadata['dataset_columns']
    stats = dict(mean=pd.DataFrame(stats['mean'][None, ...], columns=columns[:-1]),
                 std=pd.DataFrame(stats['std'][None, ...], columns=columns[:-1]))
    return dataset_metadata, pd.DataFrame(data, columns=columns), stats