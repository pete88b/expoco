{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70959a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp viseme_image.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042ebe0",
   "metadata": {},
   "source": [
    "# Image model\n",
    "\n",
    "> Conv net, trained with fastai running in onnx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5196d33f-d07f-45da-af21-fdd22aad2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from expoco.viseme_image.data import *\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2, onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b6f01",
   "metadata": {},
   "source": [
    "# Prepare data for inference\n",
    "\n",
    "We need to replicate what fastai data loaders do ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc363eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_for_inference(image, image_size=256):\n",
    "    \"Convert a cv2 style image to something that can be used as input to a CNN\"\n",
    "    if image.shape[0] > image_size:\n",
    "        image = ImageHelper().face_crop(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    image = (image/255.)\n",
    "    image = image.astype(np.float32)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "500862e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_image = np.zeros((4, 5, 3), np.uint8)\n",
    "B, G, R = 255, 125, 0\n",
    "_image[:,] = B, G, R\n",
    "_image = prepare_for_inference(_image)\n",
    "assert _image.shape == (3, 4, 5)\n",
    "assert np.allclose(_image[0], R/255)\n",
    "assert np.allclose(_image[1], G/255)\n",
    "assert np.allclose(_image[2], B/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584fc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VisemeClassifier:\n",
    "    def __init__(self, model_path, image_size=256):\n",
    "        self.model = onnxruntime.InferenceSession(str(model_path))\n",
    "        self.image_size = image_size\n",
    "        self.image_helper = ImageHelper()\n",
    "        self.vocab = ['AH', 'EE', 'NO_EXPRESSION', 'OO']\n",
    "        self.item_queue = []\n",
    "    def _to_image(self, item):\n",
    "        return self.image_helper.read_image(item) if isinstance(item, (str, Path)) else item\n",
    "    def predict(self, items=None):\n",
    "        if items is None:\n",
    "            items = self.item_queue\n",
    "            self.item_queue = []\n",
    "        else:\n",
    "            items = [self._to_image(i) for i in items]\n",
    "            items = [prepare_for_inference(i, self.image_size) for i in items]\n",
    "        items = np.array(items)\n",
    "        model_output = self.model.run(None, {'input': items})\n",
    "        output = model_output[0]\n",
    "        class_ids = np.argmax(output, axis=1)\n",
    "        class_names = [self.vocab[class_id] for class_id in class_ids]\n",
    "        return class_names\n",
    "    def queue_item(self, item):\n",
    "        self.item_queue.append(prepare_for_inference(self._to_image(item), self.image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a825bc6",
   "metadata": {},
   "source": [
    "`queue_item` prepares for inference as this prep is quite often slower that actually running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff51798",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('models/model_20211202_143854/resnet_3_256_256.onnx')\n",
    "img_path = Path('test/data/raw_images')\n",
    "imgs = [img_path/'oo_1.png', img_path/'oo_2.png', img_path/'ee_1.png', img_path/'ee_2.png']\n",
    "img_path = Path('test/data/processed_images')\n",
    "imgs += [img_path/'ah_1.png', img_path/'ah_2.png']\n",
    "viseme_classifier = VisemeClassifier(model_path)\n",
    "class_names = viseme_classifier.predict(imgs)\n",
    "for img in imgs:\n",
    "    viseme_classifier.queue_item(img)\n",
    "class_names2 = viseme_classifier.predict()\n",
    "assert class_names == class_names2 == ['OO', 'OO', 'EE', 'EE', 'AH', 'AH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ee9ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01a_camera_capture.ipynb.\n",
      "Converted 10a_viseme_tabular_identify_landmarks.ipynb.\n",
      "Converted 10b_viseme_tabular_data.ipynb.\n",
      "Converted 10d_viseme_tabular_model.ipynb.\n",
      "Converted 10e_viseme_tabular_train_model.ipynb.\n",
      "Converted 10f_viseme_tabular_test_model.ipynb.\n",
      "Converted 11b_viseme_image_data.ipynb.\n",
      "Converted 11d_viseme_image_model.ipynb.\n",
      "Converted 11e_viseme_image_train_model.ipynb.\n",
      "Converted 11f_viseme_image_test_model.ipynb.\n",
      "Converted 20a_gui_capture_command.ipynb.\n",
      "Converted 20a_gui_main.ipynb.\n",
      "Converted 70_cli.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted project_lifecycle.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
