{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0042ebe0",
   "metadata": {},
   "source": [
    "# Test cnn model\n",
    "\n",
    "> Conv net, trained with fastai running in onnx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5196d33f-d07f-45da-af21-fdd22aad2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from expoco.core import *\n",
    "from expoco.viseme_image.data import *\n",
    "from expoco.viseme_image.model import *\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "import cv2, time, math, json, shutil\n",
    "import win32api, win32con\n",
    "import onnx, onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b6f01",
   "metadata": {},
   "source": [
    "# Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc363eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('data/viseme_image_dataset_20220202_131034/model_20220202_134036/resnet_3_256_256.onnx')\n",
    "dataset_path = model_path.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c908aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(model_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08eab1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[2.8649325e-09, 3.4617631e-10, 2.3448924e-19, 1.0000000e+00]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(1, 3, 256, 256).astype(np.float32)\n",
    "ort_session = onnxruntime.InferenceSession(str(model_path))\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: x}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "ort_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da364db",
   "metadata": {},
   "source": [
    "# make sure we get the right results with images we trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573f2a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67967d04398b4cde8b0fcc35af6605aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x08\\x00\\x00\\x00\\x00:~\\x9bU\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done total 100 correct 100 acc 1.0\n"
     ]
    }
   ],
   "source": [
    "win32api.GetAsyncKeyState(win32con.VK_ESCAPE)\n",
    "viseme_classifier = VisemeClassifier(model_path)\n",
    "image_helper = ImageHelper()\n",
    "image_display_helper = ImageDisplayHelper(np.zeros([1,1]), 'expoco: test')\n",
    "vocab = ['AH', 'EE', 'NO_EXPRESSION', 'OO']\n",
    "total, correct = 0, 0\n",
    "with ZipFile(dataset_path/'data.zip') as zip_file:\n",
    "    name_list = [n for n in zip_file.namelist() if n not in ['AH/', 'EE/', 'NO_EXPRESSION/', 'OO/']]\n",
    "    for i in range(0, len(name_list), 100):\n",
    "        total+=1\n",
    "        actual = name_list[i].split('/')[0]\n",
    "        zip_file.extract(name_list[i], '/temp')\n",
    "        raw_image = cv2.imread(f'/temp/{name_list[i]}')\n",
    "        class_name = viseme_classifier.predict([raw_image])[0]\n",
    "        if actual == class_name:\n",
    "            correct += 1\n",
    "        else:\n",
    "            image_display_helper.show(\n",
    "                cv2.putText(raw_image, f'act:{actual} pred:{class_name}', (5,15), fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=.5, color=255))\n",
    "            while True:\n",
    "                time.sleep(.25)\n",
    "                if win32api.GetAsyncKeyState(win32con.VK_ESCAPE):\n",
    "                    image_display_helper.show(raw_image/2)\n",
    "                    break\n",
    "print('done total', total, 'correct', correct, 'acc', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542338bf",
   "metadata": {},
   "source": [
    "# How long will inference take\n",
    "\n",
    "worst case would be running inference one sample at a time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d715f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(str(model_path))\n",
    "one_row_x = np.random.randn(1, 3, 256, 256).astype(np.float32)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: one_row_x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c59e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 calls with 1 rows took 2.1565802097320557 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time, how_many = time.time(), 50\n",
    "for i in range(how_many):\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "print(how_many, 'calls with', one_row_x.shape[0], 'rows took', time.time()-start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8390",
   "metadata": {},
   "source": [
    "&uarr; quick (o:\n",
    "\n",
    "how much quicker would processing 2 samples at a time be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad8b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rows_x = np.random.randn(2, 3, 256, 256).astype(np.float32)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: one_row_x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3660c915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 calls with 2 rows took 2.0057129859924316 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time, how_many = time.time(), 50\n",
    "for i in range(how_many):\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "print(how_many, 'calls with', two_rows_x.shape[0], 'rows took', time.time()-start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3617c",
   "metadata": {},
   "source": [
    "&uarr; similar time for 2x as many preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af205bd",
   "metadata": {},
   "source": [
    "# Live test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b763402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _putText(image, text, org):\n",
    "    cv2.putText(image, text, org, fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=.75, color=(0,0,0), thickness=2)\n",
    "    cv2.putText(image, text, org, fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=.75, color=(255,255,255), thickness=1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac8f63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_test():\n",
    "    win32api.GetAsyncKeyState(win32con.VK_ESCAPE)\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    ort_session = onnxruntime.InferenceSession(str(model_path))\n",
    "    image_helper = VisemeClassifierImageHelper()\n",
    "    image_display_helper = ImageDisplayHelper(np.zeros([1,1]), 'expoco: Capture session')\n",
    "    vocab = ['AH', 'EE', 'NO_EXPRESSION', 'OO']\n",
    "    try:\n",
    "        _, _ = video_capture.read()\n",
    "        while True:\n",
    "            if win32api.GetAsyncKeyState(win32con.VK_ESCAPE): \n",
    "                break\n",
    "            retval, image = video_capture.read()\n",
    "            raw_image = image\n",
    "            image = image = image_helper.prepare_for_inference(image)\n",
    "            ort_inputs = {'input': image[None, ...]}\n",
    "            ort_outs = ort_session.run(None, ort_inputs)\n",
    "            output = ort_outs[0][0]\n",
    "            class_id = np.argmax(output)\n",
    "            class_name = vocab[class_id]\n",
    "            raw_image = image_helper.flip(raw_image)\n",
    "            raw_image = _putText(raw_image, f'{class_id} {class_name}', (0,20))\n",
    "            for i in range(4):\n",
    "                raw_image = _putText(raw_image, f'{np.round(output[i], 2)}', (0,40+(i*20)))\n",
    "            image_display_helper.show(raw_image)\n",
    "            time.sleep(.05)\n",
    "    finally:\n",
    "        video_capture.release()\n",
    "#         image_widget.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da879716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289ccca3ea1b4038872580902b5211a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x08\\x00\\x00\\x00\\x00:~\\x9bU\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "live_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a819d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0991ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_test2():\n",
    "    win32api.GetAsyncKeyState(win32con.VK_ESCAPE)\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    viseme_classifier = VisemeClassifier(model_path)\n",
    "    image_helper = ImageHelper()\n",
    "    image_display_helper = ImageDisplayHelper(np.zeros([1,1]), 'expoco: Capture session')\n",
    "    vocab = ['AH', 'EE', 'NO_EXPRESSION', 'OO']\n",
    "    try:\n",
    "        _, _ = video_capture.read()\n",
    "        while True:\n",
    "            if win32api.GetAsyncKeyState(win32con.VK_ESCAPE): \n",
    "                break\n",
    "            retval, image = video_capture.read()\n",
    "            viseme_classifier.queue_item(image)\n",
    "            if len(viseme_classifier.item_queue) > 1:\n",
    "                class_names = viseme_classifier.predict()\n",
    "                image = image_helper.flip(image)\n",
    "                image = _putText(image, f'{class_names}', (0,20))\n",
    "                image_display_helper.show(image)\n",
    "            time.sleep(.05)\n",
    "    finally:\n",
    "        video_capture.release()\n",
    "#         image_widget.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3001c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_test2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3b328",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "Collect images and save as .png (cropped, but NOT normalized ) - during training we'll have to pre-process and augment https://github.com/cordmaur/Fastai2-Medium/blob/master/01_Create_Datablock.ipynb and [towardsdatascience article](https://towardsdatascience.com/how-to-create-a-datablock-for-multispectral-satellite-image-segmentation-with-the-fastai-v2-bc5e82f4eb5)\n",
    "\n",
    "do better at onnx export https://github.com/tkeyo/fastai-onnx/blob/main/fastai_to_onnx.ipynb and [dev.to article](https://dev.to/tkeyo/export-fastai-resnet-models-to-onnx-2gj7)\n",
    "\n",
    "## use smallest image possible\n",
    "\n",
    "see how this influences inference time\n",
    "\n",
    "- read from raw np file - .png is smaller and loads to give the same data\n",
    "    - saving as int might be most efficient - just save as cropped grey image\n",
    "    - if so, make data prep code sharable between training/inference\n",
    "- use black and white\n",
    "- crop \n",
    "    - static crop?\n",
    "    - use face mesh to locate face and crop around it?\n",
    "        - which is faster/better: face mesh or cv2 HAAR thing\n",
    "    \n",
    "## build model that classifies viseme and regresses where face is pointing\n",
    "\n",
    "https://walkwithfastai.com/Multimodal_Head_and_Kaggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
