# AUTOGENERATED! DO NOT EDIT! File to edit: 10b_viseme_tabular_data.ipynb (unless otherwise specified).

__all__ = ['mp_face_mesh', 'viseme_tabular_dataset_from_capture_sessions', 'read_viseme_tabular_dataset',
           'get_image_path', 'make_landmarks_relative', 'change_viseme_class', 'calculate_stats', 'normalize',
           'inference_data_from_landmarks', 'processed_dataset_from_viseme_tabular_dataset', 'read_processed_dataset']

# Cell
from ..core import *
from ..camera_capture import *
from pathlib import Path
import numpy as np
import pandas as pd
import json, cv2

import mediapipe as mp
mp_face_mesh = mp.solutions.face_mesh

# Cell
def viseme_tabular_dataset_from_capture_sessions(
        input_path='data/capture_sessions',
        glob_pattern='*'):
    "Create a viseme tabular dataset from capture session images"
    input_path, dataset_id, data = Path(input_path), now(), []
    output_path = input_path.parent/f'viseme_tabular_dataset_{dataset_id}'
    output_path.mkdir()
    face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1)
    landmark_ids = sorted(FaceLandmarks.pointer + FaceLandmarks.mouth)
    metadata = dict(input_path=path_to_str(input_path), output_path=path_to_str(output_path),
                    glob_pattern=glob_pattern, session_metadata=[], start_date=now(),
                    landmark_ids=landmark_ids,
                    column_names=landmark_ids_to_col_names(FaceLandmarks.pointer + FaceLandmarks.mouth))
    data = []
    for session_path in sorted(input_path.glob(glob_pattern)):
        with open(session_path/'metadata.json') as f:
            session_metadata = json.load(f)
        metadata['session_metadata'].append(session_metadata)
        for capture_count in range(1, session_metadata['count']+1):
            image = cv2.imread(f'{session_path}/{capture_count}.png')
            results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
            row = []
            for landmark_id in landmark_ids:
                landmark = results.multi_face_landmarks[0].landmark[landmark_id]
                for coord in ['x','y']:
                    row.append(getattr(landmark, coord))
            data.append(row)
    metadata['end_date'] = now()
    with open(output_path/'metadata.json', 'w') as f: json.dump(metadata, f, indent=2)
    np.save(output_path/'data.npy', np.array(data), allow_pickle=False)
    return output_path

# Cell
def read_viseme_tabular_dataset(dataset_path, to_numpy=True):
    "Return metadata and data (as a numpy array by default, pandas dataframe if `to_numpy=False`)"
    dataset_path = Path(dataset_path)
    data = np.load(dataset_path/'data.npy')
    with open(dataset_path/'metadata.json') as f:
        dataset_metadata = json.load(f)
    viseme_class = []
    for session_metadata in dataset_metadata['session_metadata']:
        viseme_class.extend([get_viseme_class(session_metadata)]*session_metadata['count'])
    if to_numpy:
        return dataset_metadata, data, viseme_class
    df = pd.DataFrame(data, columns=dataset_metadata['column_names'])
    df = pd.concat([df, pd.DataFrame(dict(viseme_class=viseme_class))], axis=1)
    return dataset_metadata, df

# Cell
def get_image_path(dataset_path, row_idx):
    dataset_path = Path(dataset_path)
    if dataset_path.name.startswith('processed'):
        # TODO: read processed metadata and find dataset path - don't just assume its the parent
        dataset_path = dataset_path.parent
    with open(dataset_path/'metadata.json') as f:
        dataset_metadata = json.load(f)
    idx = row_idx
    for session_metadata in dataset_metadata['session_metadata']:
        if idx < session_metadata['count']:
            return Path(session_metadata['path'])/f'{idx+1}.jpeg'
        idx -= session_metadata['count']

# Internal Cell
def select_columns(dataset_metadata, data, column_names):
    return data[..., [c in column_names for c in dataset_metadata['column_names']]]

# Cell
def make_landmarks_relative(data, to_landmarks, column_count=None):
    "Make all landmarks in `data` relative `to_landmarks` in-place"
    coord_count = to_landmarks.shape[-1]
    assert coord_count == 2, f'to_landmarks must have 2 "coord_count". to_arr.shape={coord_count}'
    for i in range(coord_count):
        data[:, i:column_count:coord_count] -= to_landmarks[..., i][..., None]
    return data

# Cell
def change_viseme_class(viseme_class, from_class, to_class):
    "Change values in viseme_class from one class to another"
    return [to_class if c==from_class else c for c in viseme_class]

# Cell
def calculate_stats(data):
    "Return the mean and standard deviation over columns of `data`"
    return dict(mean=data.mean(axis=0), std=data.std(axis=0))

# Cell
def normalize(data, mean, std):
    column_count = len(mean) # use len rather than shape[0] so that mean stats can be lists or np arrays
    data[..., :column_count] -= mean
    data[..., :column_count] /= std
    return data

# Cell
def inference_data_from_landmarks(landmarks, landmark_ids, relative_landmark_id=None, coords=['x', 'y'], stats=None):
    landmark_ids, coords, data, to_landmarks_data = sorted(landmark_ids), sorted(coords), [], []
    for landmark_id in landmark_ids:
        for coord in coords:
            if landmark_id == relative_landmark_id:
                to_landmarks_data.append(getattr(landmarks[landmark_id], coord))
            else:
                data.append(getattr(landmarks[landmark_id], coord))
    data, to_landmarks_data = np.array([data], dtype=float), np.array([to_landmarks_data], dtype=float)
    if relative_landmark_id is not None:
        make_landmarks_relative(data, to_landmarks_data)
    if stats is not None:
        normalize(data, **stats)
    return data

# Cell
def processed_dataset_from_viseme_tabular_dataset(
        input_path, relative_landmark_id=None, change_y_from=None, change_y_to=None):
    "Create a processed dataset (ready for ML) from a viseme tabular dataset"
    input_path = Path(input_path)
    input_metadata, input_data, input_y = read_viseme_tabular_dataset(input_path)
    output_path = Path(input_metadata['output_path'])/f'processed_{now()}'
    output_path.mkdir()
    column_names = landmark_ids_to_col_names(input_metadata['landmark_ids'], relative_landmark_id)
    metadata = dict(input_path=path_to_str(input_path),
                    input_metadata=input_metadata,
                    output_path=path_to_str(output_path),
                    relative_landmark_id=relative_landmark_id,
                    change_y_from=change_y_from,
                    change_y_to=change_y_to,
                    column_names=column_names,
                    start_date=now())
    data = select_columns(input_metadata, input_data, column_names)
    if change_y_from is not None:
        assert change_y_to is not None
        input_y = change_viseme_class(input_y, change_y_from, change_y_to)
    metadata['viseme_class'] = input_y
    if relative_landmark_id is not None:
        relative_names = landmark_ids_to_col_names([relative_landmark_id])
        metadata['relative_names']=relative_names
        to_landmarks = select_columns(input_metadata, input_data, relative_names)
        make_landmarks_relative(data, to_landmarks)
    stats = calculate_stats(data)
    normalize(data, **stats)
    metadata['end_date'] = now()
    with open(output_path/'metadata.json', 'w') as f: json.dump(metadata, f, indent=2)
    np.save(output_path/'data.npy', data, allow_pickle=False)
    np.savez(output_path/'stats.npz', **stats)
    return output_path

# Cell
def read_processed_dataset(dataset_path, to_numpy=True):
    "Return metadata, data and stats (as a numpy array by default, pandas dataframe if `to_numpy=False`)"
    dataset_path = Path(dataset_path)
    data, stats = np.load(dataset_path/'data.npy'), np.load(dataset_path/'stats.npz')
    with open(dataset_path/'metadata.json') as f:
        dataset_metadata = json.load(f)
    viseme_class = dataset_metadata['viseme_class']
    if to_numpy:
        return dataset_metadata, data, viseme_class, stats
    column_names = dataset_metadata['column_names']
    stats = dict(mean=pd.DataFrame(stats['mean'][None, ...], columns=column_names),
                 std=pd.DataFrame(stats['std'][None, ...], columns=column_names))
    df = pd.DataFrame(data, columns=dataset_metadata['column_names'])
    df = pd.concat([df, pd.DataFrame(dict(viseme_class=viseme_class))], axis=1)
    return dataset_metadata, df, stats