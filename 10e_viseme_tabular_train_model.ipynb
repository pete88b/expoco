{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a viseme classifier\n",
    "\n",
    "> Train a viseme classifier in colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73fVyAu3MfLE"
   },
   "source": [
    "# TODO\n",
    "- overview of pre-requisits\n",
    "\n",
    "combine 0:no expression and 4:random into a single 0:ignore class\n",
    "\n",
    "at inference time, we don't care about the difference\n",
    "\n",
    "the class imbalance might also be useful - as this will be our do nothing default - and we only want to take action when we're sure a command is being requested\n",
    "\n",
    "TODO: \n",
    "- save stats with model\n",
    "- Try bigger batch size - even if it means we need more epochs\n",
    "- model size/depth\n",
    "- Exaggerate 0 imbalance - how far do we have to go to not got any 0s misclassified (on test set)\n",
    "- see what happens if we don't use nose landmarks for training - try with/without normalizing for nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9dAM3V0MfLK",
    "outputId": "6018c326-87c3-49cf-8195-e67b44c149ad"
   },
   "outputs": [],
   "source": [
    "# run this, then restart the runtime\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNOQTw1pNKJa",
    "outputId": "1610a5de-ff48-40d1-c435-9645b1262062"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnIxWYdgMfLL"
   },
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UnIxWYdgMfLL"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F6UXJETwMfLM"
   },
   "outputs": [],
   "source": [
    "path = Path('data/viseme_dataset_20211113_114625/processed_20211113_145138')\n",
    "# path = Path('/content/drive/MyDrive/Colab Notebooks/datasets/expoco/processed_20211019_205023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from data\\viseme_dataset_20211113_114625\\processed_20211113_145138\n",
      "data.shape (2000, 213)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5x</th>\n",
       "      <th>5y</th>\n",
       "      <th>2x</th>\n",
       "      <th>2y</th>\n",
       "      <th>218x</th>\n",
       "      <th>218y</th>\n",
       "      <th>438x</th>\n",
       "      <th>438y</th>\n",
       "      <th>0x</th>\n",
       "      <th>0y</th>\n",
       "      <th>...</th>\n",
       "      <th>408y</th>\n",
       "      <th>409x</th>\n",
       "      <th>409y</th>\n",
       "      <th>410x</th>\n",
       "      <th>410y</th>\n",
       "      <th>415x</th>\n",
       "      <th>415y</th>\n",
       "      <th>424x</th>\n",
       "      <th>424y</th>\n",
       "      <th>expression_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.505646</td>\n",
       "      <td>0.494332</td>\n",
       "      <td>0.503341</td>\n",
       "      <td>0.490903</td>\n",
       "      <td>0.498368</td>\n",
       "      <td>0.501670</td>\n",
       "      <td>0.506184</td>\n",
       "      <td>0.492390</td>\n",
       "      <td>0.507021</td>\n",
       "      <td>0.487174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466999</td>\n",
       "      <td>0.496324</td>\n",
       "      <td>0.469936</td>\n",
       "      <td>0.502941</td>\n",
       "      <td>0.464381</td>\n",
       "      <td>0.496418</td>\n",
       "      <td>0.444797</td>\n",
       "      <td>0.493698</td>\n",
       "      <td>0.493209</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.503841</td>\n",
       "      <td>0.494711</td>\n",
       "      <td>0.501086</td>\n",
       "      <td>0.488799</td>\n",
       "      <td>0.499251</td>\n",
       "      <td>0.501959</td>\n",
       "      <td>0.504125</td>\n",
       "      <td>0.492880</td>\n",
       "      <td>0.504448</td>\n",
       "      <td>0.487415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466239</td>\n",
       "      <td>0.493961</td>\n",
       "      <td>0.467831</td>\n",
       "      <td>0.499657</td>\n",
       "      <td>0.463201</td>\n",
       "      <td>0.489543</td>\n",
       "      <td>0.439547</td>\n",
       "      <td>0.495851</td>\n",
       "      <td>0.491819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.496112</td>\n",
       "      <td>0.494247</td>\n",
       "      <td>0.497164</td>\n",
       "      <td>0.492464</td>\n",
       "      <td>0.500322</td>\n",
       "      <td>0.503856</td>\n",
       "      <td>0.495550</td>\n",
       "      <td>0.492506</td>\n",
       "      <td>0.494844</td>\n",
       "      <td>0.488023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476045</td>\n",
       "      <td>0.482862</td>\n",
       "      <td>0.479148</td>\n",
       "      <td>0.487478</td>\n",
       "      <td>0.472226</td>\n",
       "      <td>0.475334</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.493696</td>\n",
       "      <td>0.497160</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.489471</td>\n",
       "      <td>0.498384</td>\n",
       "      <td>0.493200</td>\n",
       "      <td>0.493677</td>\n",
       "      <td>0.501554</td>\n",
       "      <td>0.505352</td>\n",
       "      <td>0.488306</td>\n",
       "      <td>0.496586</td>\n",
       "      <td>0.486446</td>\n",
       "      <td>0.491592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479880</td>\n",
       "      <td>0.472891</td>\n",
       "      <td>0.484327</td>\n",
       "      <td>0.478250</td>\n",
       "      <td>0.475697</td>\n",
       "      <td>0.458374</td>\n",
       "      <td>0.454175</td>\n",
       "      <td>0.489752</td>\n",
       "      <td>0.500201</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.487420</td>\n",
       "      <td>0.497972</td>\n",
       "      <td>0.490142</td>\n",
       "      <td>0.493929</td>\n",
       "      <td>0.499240</td>\n",
       "      <td>0.506324</td>\n",
       "      <td>0.486311</td>\n",
       "      <td>0.496072</td>\n",
       "      <td>0.484130</td>\n",
       "      <td>0.491305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483028</td>\n",
       "      <td>0.463874</td>\n",
       "      <td>0.487154</td>\n",
       "      <td>0.469231</td>\n",
       "      <td>0.478301</td>\n",
       "      <td>0.451558</td>\n",
       "      <td>0.452418</td>\n",
       "      <td>0.485923</td>\n",
       "      <td>0.501094</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-2.283781</td>\n",
       "      <td>-2.233636</td>\n",
       "      <td>-2.284498</td>\n",
       "      <td>-2.249344</td>\n",
       "      <td>-2.286113</td>\n",
       "      <td>-2.278023</td>\n",
       "      <td>-2.283565</td>\n",
       "      <td>-2.230405</td>\n",
       "      <td>-2.283090</td>\n",
       "      <td>-2.227529</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.224948</td>\n",
       "      <td>-2.249833</td>\n",
       "      <td>-2.229237</td>\n",
       "      <td>-2.260420</td>\n",
       "      <td>-2.224020</td>\n",
       "      <td>-2.249427</td>\n",
       "      <td>-2.208283</td>\n",
       "      <td>-2.269798</td>\n",
       "      <td>-2.259759</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-2.323838</td>\n",
       "      <td>-2.271298</td>\n",
       "      <td>-2.324637</td>\n",
       "      <td>-2.289553</td>\n",
       "      <td>-2.326421</td>\n",
       "      <td>-2.322912</td>\n",
       "      <td>-2.323605</td>\n",
       "      <td>-2.267545</td>\n",
       "      <td>-2.323097</td>\n",
       "      <td>-2.264216</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.261390</td>\n",
       "      <td>-2.285296</td>\n",
       "      <td>-2.266389</td>\n",
       "      <td>-2.297457</td>\n",
       "      <td>-2.260306</td>\n",
       "      <td>-2.285031</td>\n",
       "      <td>-2.242138</td>\n",
       "      <td>-2.307770</td>\n",
       "      <td>-2.301655</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-2.352946</td>\n",
       "      <td>-2.263165</td>\n",
       "      <td>-2.353804</td>\n",
       "      <td>-2.280870</td>\n",
       "      <td>-2.355710</td>\n",
       "      <td>-2.313218</td>\n",
       "      <td>-2.352699</td>\n",
       "      <td>-2.259525</td>\n",
       "      <td>-2.352167</td>\n",
       "      <td>-2.256293</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.253520</td>\n",
       "      <td>-2.311065</td>\n",
       "      <td>-2.258367</td>\n",
       "      <td>-2.324370</td>\n",
       "      <td>-2.252470</td>\n",
       "      <td>-2.310902</td>\n",
       "      <td>-2.234828</td>\n",
       "      <td>-2.335362</td>\n",
       "      <td>-2.292608</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-2.372248</td>\n",
       "      <td>-2.264530</td>\n",
       "      <td>-2.373145</td>\n",
       "      <td>-2.282327</td>\n",
       "      <td>-2.375134</td>\n",
       "      <td>-2.314845</td>\n",
       "      <td>-2.371993</td>\n",
       "      <td>-2.260871</td>\n",
       "      <td>-2.371445</td>\n",
       "      <td>-2.257623</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.254841</td>\n",
       "      <td>-2.328153</td>\n",
       "      <td>-2.259713</td>\n",
       "      <td>-2.342218</td>\n",
       "      <td>-2.253785</td>\n",
       "      <td>-2.328059</td>\n",
       "      <td>-2.236054</td>\n",
       "      <td>-2.353659</td>\n",
       "      <td>-2.294126</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-2.394240</td>\n",
       "      <td>-2.287025</td>\n",
       "      <td>-2.395182</td>\n",
       "      <td>-2.306344</td>\n",
       "      <td>-2.397263</td>\n",
       "      <td>-2.341657</td>\n",
       "      <td>-2.393975</td>\n",
       "      <td>-2.283055</td>\n",
       "      <td>-2.393409</td>\n",
       "      <td>-2.279535</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.276607</td>\n",
       "      <td>-2.347623</td>\n",
       "      <td>-2.281904</td>\n",
       "      <td>-2.362551</td>\n",
       "      <td>-2.275459</td>\n",
       "      <td>-2.347606</td>\n",
       "      <td>-2.256276</td>\n",
       "      <td>-2.374506</td>\n",
       "      <td>-2.319151</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            5x        5y        2x        2y      218x      218y      438x  \\\n",
       "0     0.505646  0.494332  0.503341  0.490903  0.498368  0.501670  0.506184   \n",
       "1     0.503841  0.494711  0.501086  0.488799  0.499251  0.501959  0.504125   \n",
       "2     0.496112  0.494247  0.497164  0.492464  0.500322  0.503856  0.495550   \n",
       "3     0.489471  0.498384  0.493200  0.493677  0.501554  0.505352  0.488306   \n",
       "4     0.487420  0.497972  0.490142  0.493929  0.499240  0.506324  0.486311   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1995 -2.283781 -2.233636 -2.284498 -2.249344 -2.286113 -2.278023 -2.283565   \n",
       "1996 -2.323838 -2.271298 -2.324637 -2.289553 -2.326421 -2.322912 -2.323605   \n",
       "1997 -2.352946 -2.263165 -2.353804 -2.280870 -2.355710 -2.313218 -2.352699   \n",
       "1998 -2.372248 -2.264530 -2.373145 -2.282327 -2.375134 -2.314845 -2.371993   \n",
       "1999 -2.394240 -2.287025 -2.395182 -2.306344 -2.397263 -2.341657 -2.393975   \n",
       "\n",
       "          438y        0x        0y  ...      408y      409x      409y  \\\n",
       "0     0.492390  0.507021  0.487174  ...  0.466999  0.496324  0.469936   \n",
       "1     0.492880  0.504448  0.487415  ...  0.466239  0.493961  0.467831   \n",
       "2     0.492506  0.494844  0.488023  ...  0.476045  0.482862  0.479148   \n",
       "3     0.496586  0.486446  0.491592  ...  0.479880  0.472891  0.484327   \n",
       "4     0.496072  0.484130  0.491305  ...  0.483028  0.463874  0.487154   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1995 -2.230405 -2.283090 -2.227529  ... -2.224948 -2.249833 -2.229237   \n",
       "1996 -2.267545 -2.323097 -2.264216  ... -2.261390 -2.285296 -2.266389   \n",
       "1997 -2.259525 -2.352167 -2.256293  ... -2.253520 -2.311065 -2.258367   \n",
       "1998 -2.260871 -2.371445 -2.257623  ... -2.254841 -2.328153 -2.259713   \n",
       "1999 -2.283055 -2.393409 -2.279535  ... -2.276607 -2.347623 -2.281904   \n",
       "\n",
       "          410x      410y      415x      415y      424x      424y  \\\n",
       "0     0.502941  0.464381  0.496418  0.444797  0.493698  0.493209   \n",
       "1     0.499657  0.463201  0.489543  0.439547  0.495851  0.491819   \n",
       "2     0.487478  0.472226  0.475334  0.449275  0.493696  0.497160   \n",
       "3     0.478250  0.475697  0.458374  0.454175  0.489752  0.500201   \n",
       "4     0.469231  0.478301  0.451558  0.452418  0.485923  0.501094   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "1995 -2.260420 -2.224020 -2.249427 -2.208283 -2.269798 -2.259759   \n",
       "1996 -2.297457 -2.260306 -2.285031 -2.242138 -2.307770 -2.301655   \n",
       "1997 -2.324370 -2.252470 -2.310902 -2.234828 -2.335362 -2.292608   \n",
       "1998 -2.342218 -2.253785 -2.328059 -2.236054 -2.353659 -2.294126   \n",
       "1999 -2.362551 -2.275459 -2.347606 -2.256276 -2.374506 -2.319151   \n",
       "\n",
       "      expression_id  \n",
       "0               0.0  \n",
       "1               0.0  \n",
       "2               0.0  \n",
       "3               0.0  \n",
       "4               0.0  \n",
       "...             ...  \n",
       "1995            0.0  \n",
       "1996            0.0  \n",
       "1997            0.0  \n",
       "1998            0.0  \n",
       "1999            0.0  \n",
       "\n",
       "[2000 rows x 213 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('loading from', path)\n",
    "data = np.load(path/'data.npy')\n",
    "print('data.shape', data.shape)\n",
    "with open(path/'metadata.json') as f:\n",
    "    metadata = json.load(f)\n",
    "df = pd.DataFrame(data, columns=metadata['cont_names'] + [metadata['y_name']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCLk_dVnMfLS"
   },
   "outputs": [],
   "source": [
    "dls = TabularDataLoaders.from_df(df, path, procs=[], cat_names=[], cont_names=list(df.columns[:-1]), \n",
    "                                 y_names=df.columns[-1], y_block=CategoryBlock,\n",
    "                                 valid_idx=list(range(0, len(df), 10)), bs=256)\n",
    "learn = tabular_learner(dls, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "Vzhh4FiFMfLT",
    "outputId": "6905eac1-a28a-4ecc-8aff-9bd34a090481"
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaoV3-HpNr5K"
   },
   "source": [
    "# relative to tip of nose\n",
    "```\n",
    "bs 64\n",
    "25, 8e-4 -> 97.4 + wd=1e-2 -> 97.7\n",
    "bs 256\n",
    "25, 8e-4, wd=1e-2 -> 98.1-98.4\n",
    "bs 1024\n",
    "35, 8e-4, wd=1e-2 -> 97.7\n",
    "```\n",
    "# relative to top lip indent\n",
    "\n",
    "slighly lower accuracy but might give us better results if we care about not misclassifying no expression\n",
    "\n",
    "```\n",
    "bs 256\n",
    "25, 8e-4, wd=1e-2 -> 97.3\n",
    "25, 8e-4, wd=8e-3 -> 97.8\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "id": "sB9kOU-1MfLT",
    "outputId": "2a8ddc11-7b10-4a3d-ef90-ecfd1179ba7f"
   },
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls, metrics=accuracy)\n",
    "learn.fit_one_cycle(25, 1e-3, wd=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "SrVZcH8rMfLU",
    "outputId": "30d6a01f-f432-4053-a014-d69477d537d9"
   },
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uCCGulHMfLU",
    "outputId": "9f335c11-462c-41a3-f35d-44a97811dec6"
   },
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-BRGDd0MfLV"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "class NpModelHelper:\n",
    "    def __init__(self, path, model_id):\n",
    "        self.model_id, self.path = model_id, path/f'model_{model_id}.pkl'\n",
    "    def set_state(self, model):\n",
    "        self.state_dict, state_dict = {}, learn.model.state_dict()\n",
    "        for k in state_dict:\n",
    "            self.state_dict[k] = state_dict[k].detach().cpu().numpy()\n",
    "        return self\n",
    "    def save(self):\n",
    "        # can't set allow_pickle=False with np.savez https://github.com/numpy/numpy/issues/13983\n",
    "        # so we might as well pickle and keep the dict order\n",
    "        # TODO: don't need the dict order any more - go back to npz\n",
    "        with open(self.path, 'wb') as f:\n",
    "            pickle.dump(self.state_dict, f)\n",
    "        return self\n",
    "    def load(self):\n",
    "        with open(self.path, 'rb') as f:\n",
    "            self.state_dict = pickle.load(f)\n",
    "        return self\n",
    "    def get_state_dict(self, name_prefix):\n",
    "        if name_prefix is None or name_prefix == '': \n",
    "            return self.state_dict\n",
    "        return {k[len(name_prefix)+1:]:self.state_dict[k] \n",
    "                for k in self.state_dict \n",
    "                if k.startswith(name_prefix)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ywff_3YCMfLV",
    "outputId": "b1050496-8ec7-4f00-8d85-db118f538389"
   },
   "outputs": [],
   "source": [
    "np_model_helper = NpModelHelper(path, dataset_id)\n",
    "np_model_helper.set_state(learn.model).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hGlzH5wMfLV"
   },
   "outputs": [],
   "source": [
    "np_model_helper = NpModelHelper(path, dataset_id).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfqDaltgMfLW"
   },
   "outputs": [],
   "source": [
    "class NpBatchNorm1d:\n",
    "    # https://github.com/pytorch/pytorch/blob/420b37f3c67950ed93cd8aa7a12e673fcfc5567b/aten/src/ATen/native/Normalization.cpp#L61-L126\n",
    "    def __init__(self, weight, bias, running_mean, running_var, num_batches_tracked=None):\n",
    "        self.weight, self.bias = weight, bias\n",
    "        self.running_mean, self.running_std = running_mean, np.sqrt(running_var + 1e-5)\n",
    "    def __call__(self, x):\n",
    "        x = x - self.running_mean\n",
    "        x = x / self.running_std\n",
    "        x = x * self.weight\n",
    "        x = x + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMsqH2i9MfLW"
   },
   "outputs": [],
   "source": [
    "class NpLinear:\n",
    "    def __init__(self, weight, bias=None):\n",
    "        self.weight, self.bias = weight.T, bias\n",
    "    def __call__(self, x):\n",
    "        x = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXjOUl1zMfLW"
   },
   "outputs": [],
   "source": [
    "class NpReLU:\n",
    "    def __call__(self, x):\n",
    "        return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wudm2sGRMfLX"
   },
   "outputs": [],
   "source": [
    "class NpModel:\n",
    "    def __init__(self, modules):\n",
    "        self.modules = modules\n",
    "    def __call__(self, x):\n",
    "        for module in self.modules:\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5RpyDC0MfLV"
   },
   "outputs": [],
   "source": [
    "np_model = NpModel([\n",
    "                    NpBatchNorm1d(**np_model_helper.get_state_dict('bn_cont')),\n",
    "                    NpLinear(**np_model_helper.get_state_dict('layers.0.0')),\n",
    "                    NpReLU(),\n",
    "                    NpBatchNorm1d(**np_model_helper.get_state_dict('layers.0.2')),\n",
    "                    NpLinear(**np_model_helper.get_state_dict('layers.1.0')),\n",
    "                    NpReLU(),\n",
    "                    NpBatchNorm1d(**np_model_helper.get_state_dict('layers.1.2')),\n",
    "                    NpLinear(**np_model_helper.get_state_dict('layers.2.0'))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBh3e_nRSli_",
    "outputId": "2b8ed843-aa84-40a6-a3d0-9e64c794b726"
   },
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBQEXgkUMfLU"
   },
   "outputs": [],
   "source": [
    "class_id_to_label = {\n",
    "        0: \"No expression\",\n",
    "        1: \"oo\",\n",
    "        2: \"ee\",\n",
    "        3: \"ah\",\n",
    "        4: \"Random Talking\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SG2GoBr-MfLU"
   },
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros([5,5], dtype=int) # TODO: don't hard code\n",
    "output = np_model(df[cont_names].to_numpy())\n",
    "preds = np.argmax(output, axis=1)\n",
    "targets = df['expression_id'].to_numpy()\n",
    "for p,t in zip(preds, targets):\n",
    "    confusion_matrix[t][p]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrkoIa-zMfLV",
    "outputId": "fc700a7e-899f-4bd2-8b22-4c4d1401b360"
   },
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "-xyMgZLNMfLV",
    "outputId": "8a2667c9-8055-43af-8064-8ef617225234"
   },
   "outputs": [],
   "source": [
    "# confusion_matrix = np.array([[0,2,1],[1,2,0],[2,1,0]])\n",
    "fig, ax = plt.subplots(figsize=(9,9))\n",
    "ax.matshow(confusion_matrix, cmap=plt.cm.Blues, alpha=0.8)\n",
    "# ax.xaxis.set_ticks_position('bottom') # must be after matshow\n",
    "for i in range(confusion_matrix.shape[0]):\n",
    "    for j in range(confusion_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=confusion_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "tick_marks = np.arange(5)\n",
    "# plt.xticks(tick_marks, self.data.y.classes, rotation=90)\n",
    "plt.xticks(tick_marks, class_id_to_label.values(), rotation=90)\n",
    "plt.yticks(tick_marks, class_id_to_label.values(), rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "bWaB3H0xVb42",
    "outputId": "3829ce13-2a11-49ee-c2a1-571ca30ab6ec"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "10c_mediapipe_face_mesh_train_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
