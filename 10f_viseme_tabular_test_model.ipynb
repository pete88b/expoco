{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0042ebe0",
   "metadata": {},
   "source": [
    "# Test np model\n",
    "\n",
    "> See if a numpy model, using state trained with fastai/numpy is any good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5196d33f-d07f-45da-af21-fdd22aad2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from expoco.core import *\n",
    "from expoco.viseme_tabular.data import *\n",
    "from expoco.viseme_tabular.model import *\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2, time, math, json, shutil\n",
    "import win32api, win32con\n",
    "import mediapipe as mp\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b6f01",
   "metadata": {},
   "source": [
    "# Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d5ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = Path('test/data/viseme_tabular_dataset_20211116_113131/processed_20211117_200746/model_20211117_201151')\n",
    "# relative_landmark_id=FaceLandmarks.tip_of_nose\n",
    "# model = load_tabular_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c908aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = Path('data/viseme_dataset_20211115_144008/processed_20211115_144617/model_20211115_144950')\n",
    "model_path = Path('data/viseme_tabular_dataset_20211130_163506/processed_20211130_171906/model_20211130_172300')\n",
    "relative_landmark_id=FaceLandmarks.tip_of_nose\n",
    "# model_path = Path('data/viseme_dataset_20211116_113131/processed_20211117_200746/model_20211117_201151')\n",
    "# relative_landmark_id=None # tried model without making points relative - didn't really work\n",
    "model = load_tabular_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542338bf",
   "metadata": {},
   "source": [
    "# How long will inference take\n",
    "\n",
    "worst case would be running inference one sample at a time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ac251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = model.modules[0].weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d715f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row_x, one_row_y = np.random.random([1,input_size]), np.random.randint([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c59e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 calls with 1 rows took 0.2810027599334717 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time, how_many = time.time(), 1000\n",
    "for i in range(how_many):\n",
    "    model(one_row_x)\n",
    "print(how_many, 'calls with', one_row_x.shape[0], 'rows took', time.time()-start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8390",
   "metadata": {},
   "source": [
    "&uarr; easily quick enough to not worry about.\n",
    "\n",
    "out of interest, how much quicker would processing 2 samples at a time be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad8b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rows_x, two_rows_y = np.random.random([2,input_size]), np.random.randint([2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3660c915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 calls with 2 rows took 0.08499741554260254 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time, how_many = time.time(), 500\n",
    "for i in range(how_many):\n",
    "    model(two_rows_x)\n",
    "print(how_many, 'calls with', two_rows_x.shape[0], 'rows took', time.time()-start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3617c",
   "metadata": {},
   "source": [
    "&uarr; nearly 4x quicker!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af205bd",
   "metadata": {},
   "source": [
    "# Live test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cbf8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _capture_and_process(video_capture, face_mesh): # TODO: DRY\n",
    "    retval, image = video_capture.read() # TODO: check retval\n",
    "    image = cv2.flip(image, 1)\n",
    "    return image, face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49e839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_image(image, image_widget, text, text2=None): # TODO: DRY\n",
    "    image = cv2.putText(image, text, (20,40), cv2.FONT_HERSHEY_COMPLEX, 1, (200,200,200))\n",
    "    if text2 is not None:\n",
    "        image = cv2.putText(image, text2, (20,60), cv2.FONT_HERSHEY_COMPLEX, .5, (200,200,200))\n",
    "    image_widget.value = cv2.imencode('.png', image)[1].tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "563a3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path.parent/'metadata.json') as f:\n",
    "    metadata = json.load(f)\n",
    "column_names = metadata['column_names']\n",
    "y_name = 'viseme_class'\n",
    "stats = np.load(model_path.parent/'stats.npz')\n",
    "vocab = ['AH', 'EE', 'NO_EXPRESSION', 'OO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac8f63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_test():\n",
    "    win32api.GetAsyncKeyState(win32con.VK_ESCAPE)\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    try:\n",
    "        face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1)\n",
    "        image, results = _capture_and_process(video_capture, face_mesh)\n",
    "        image_widget = widgets.Image(value=cv2.imencode('.png', image)[1].tobytes())\n",
    "        display(image_widget)\n",
    "        while True:\n",
    "            if win32api.GetAsyncKeyState(win32con.VK_ESCAPE): \n",
    "                break\n",
    "            image, results = _capture_and_process(video_capture, face_mesh)\n",
    "            if results.multi_face_landmarks is None:\n",
    "                _update_image(image, image_widget, 'No face found')\n",
    "            else:\n",
    "                data = inference_data_from_landmarks(\n",
    "                        landmarks=results.multi_face_landmarks[0].landmark, \n",
    "                        landmark_ids=FaceLandmarks.pointer + FaceLandmarks.mouth,\n",
    "                        relative_landmark_id=relative_landmark_id, \n",
    "                        coords=['x', 'y'], \n",
    "                        stats=stats)\n",
    "                output = model(data)\n",
    "                class_id = np.argmax(output)\n",
    "                class_label = vocab[class_id]\n",
    "                _update_image(image, image_widget, f'{class_id}: {class_label}', f'{np.round(output,2)[0]}')\n",
    "            time.sleep(.05)\n",
    "    finally:\n",
    "        video_capture.release()\n",
    "#         image_widget.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da879716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4bd8e699f343cd91074c95a12ce2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02\\x80\\x00\\x00\\x01\\xe0\\x08\\x02\\x00\\x00\\x00\\xba\\xb3Kâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "live_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3b328",
   "metadata": {},
   "source": [
    "# Inference using saved images of different quality\n",
    "\n",
    "See: `_tmp_re_process_image_to_face_mesh.ipynb`\n",
    "\n",
    "TODO: move &uarr; logic to this notebook - or add some static data to the test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f156cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = Path('/Users/Butterp/AppData/Local/Temp/tmpkrplc969')\n",
    "face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1)\n",
    "viseme_config = VisemeConfig()\n",
    "for i in range(0,110,10):\n",
    "    img_name = f'img_{i}.jpeg'\n",
    "    image = cv2.imread(f'{temp_path}/{img_name}')\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_face_landmarks is None:\n",
    "        print(img_name, 'no face found')\n",
    "    else:\n",
    "        data = inference_data_from_landmarks(\n",
    "                landmarks=results.multi_face_landmarks[0].landmark, \n",
    "                landmark_ids=FaceLandmarks.pointer + FaceLandmarks.mouth,\n",
    "                relative_landmark_id=FaceLandmarks.tip_of_nose, \n",
    "                coords=['x', 'y'], \n",
    "                stats=stats)\n",
    "        output = model(data)\n",
    "        class_id = np.argmax(output)\n",
    "        class_label = viseme_config.get_class_label(class_id)\n",
    "        print(img_name, class_label, class_id, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c5e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
