{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0042ebe0",
   "metadata": {},
   "source": [
    "# Test np model\n",
    "\n",
    "> See if a numpy model, using state trained with fastai/numpy is any good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5196d33f-d07f-45da-af21-fdd22aad2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from expoco.core import *\n",
    "from expoco.ml.data import *\n",
    "from expoco.ml.model import *\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2, time, math, json, shutil\n",
    "import win32api, win32con\n",
    "import mediapipe as mp\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b6f01",
   "metadata": {},
   "source": [
    "# Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c908aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = Path('data/viseme_dataset_20211115_144008/processed_20211115_144617/model_20211115_144950')\n",
    "model_path = Path('data/viseme_dataset_20211116_113131/processed_20211116_131807/model_20211116_132150')\n",
    "relative_landmark_id=FaceLandmarks.tip_of_nose\n",
    "# model_path = Path('data/viseme_dataset_20211116_113131/processed_20211117_200746/model_20211117_201151')\n",
    "# relative_landmark_id=None # tried model without making points relative - didn't really work\n",
    "model = load_tabular_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542338bf",
   "metadata": {},
   "source": [
    "# How long will inference take\n",
    "\n",
    "worst case would be running inference one sample at a time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row_x, one_row_y = np.random.random([1,212]), np.random.randint([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c59e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, how_many = time.time(), 1000\n",
    "for i in range(how_many):\n",
    "    model(one_row_x)\n",
    "print(how_many, 'calls with', one_row_x.shape[0], 'rows took', time.time()-start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8390",
   "metadata": {},
   "source": [
    "&uarr; easily quick enough to not worry about.\n",
    "\n",
    "out of interest, how much quicker would processing 2 samples at a time be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_rows_x, two_rows_y = np.random.random([2,212]), np.random.randint([2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time, how_many = time.time(), 500\n",
    "for i in range(how_many):\n",
    "    model(two_rows_x)\n",
    "print(how_many, 'calls with', two_rows_x.shape[0], 'rows took', time.time()-start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3617c",
   "metadata": {},
   "source": [
    "&uarr; nearly 4x quicker!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af205bd",
   "metadata": {},
   "source": [
    "# Live test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbf8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _capture_and_process(video_capture, face_mesh): # TODO: DRY\n",
    "    retval, image = video_capture.read() # TODO: check retval\n",
    "    image = cv2.flip(image, 1)\n",
    "    return image, face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49e839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_image(image, image_widget, text, text2=None): # TODO: DRY\n",
    "    image = cv2.putText(image, text, (20,40), cv2.FONT_HERSHEY_COMPLEX, 1, (200,200,200))\n",
    "    if text2 is not None:\n",
    "        image = cv2.putText(image, text2, (20,60), cv2.FONT_HERSHEY_COMPLEX, .5, (200,200,200))\n",
    "    image_widget.value = cv2.imencode('.png', image)[1].tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "563a3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path.parent/'metadata.json') as f:\n",
    "    metadata = json.load(f)\n",
    "cont_names = metadata['cont_names']\n",
    "y_name = metadata['y_name']\n",
    "stats = np.load(model_path.parent/'stats.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac8f63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_test():\n",
    "    win32api.GetAsyncKeyState(win32con.VK_ESCAPE)\n",
    "    viseme_config = VisemeConfig()\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    try:\n",
    "        face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1)\n",
    "        image, results = _capture_and_process(video_capture, face_mesh)\n",
    "        image_widget = widgets.Image(value=cv2.imencode('.png', image)[1].tobytes())\n",
    "        display(image_widget)\n",
    "        while True:\n",
    "            if win32api.GetAsyncKeyState(win32con.VK_ESCAPE): \n",
    "                break\n",
    "            image, results = _capture_and_process(video_capture, face_mesh)\n",
    "            if results.multi_face_landmarks is None:\n",
    "                _update_image(image, image_widget, 'No face found')\n",
    "            else:\n",
    "                data = inference_data_from_landmarks(\n",
    "                        landmarks=results.multi_face_landmarks[0].landmark, \n",
    "                        landmark_ids=FaceLandmarks.pointer + FaceLandmarks.mouth,\n",
    "                        relative_landmark_id=relative_landmark_id, \n",
    "                        coords=['x', 'y'], \n",
    "                        stats=stats)\n",
    "                output = model(data)\n",
    "                class_id = np.argmax(output)\n",
    "                class_label = viseme_config.get_class_label(class_id)\n",
    "                _update_image(image, image_widget, f'{class_id}: {class_label}', f'{np.round(output,2)[0]}')\n",
    "            time.sleep(.05)\n",
    "    finally:\n",
    "        video_capture.release()\n",
    "#         image_widget.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da879716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690f14349f1e4083964f85a48fc8a6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02\\x80\\x00\\x00\\x01\\xe0\\x08\\x02\\x00\\x00\\x00\\xba\\xb3Kâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "live_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3b328",
   "metadata": {},
   "source": [
    "# Inference using saved images of different quality\n",
    "\n",
    "See: `_tmp_re_process_image_to_face_mesh.ipynb`\n",
    "\n",
    "TODO: move &uarr; logic to this notebook - or add some static data to the test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f156cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = Path('/Users/Butterp/AppData/Local/Temp/tmpkrplc969')\n",
    "face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1)\n",
    "viseme_config = VisemeConfig()\n",
    "for i in range(0,110,10):\n",
    "    img_name = f'img_{i}.jpeg'\n",
    "    image = cv2.imread(f'{temp_path}/{img_name}')\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_face_landmarks is None:\n",
    "        print(img_name, 'no face found')\n",
    "    else:\n",
    "        data = inference_data_from_landmarks(\n",
    "                landmarks=results.multi_face_landmarks[0].landmark, \n",
    "                landmark_ids=FaceLandmarks.pointer + FaceLandmarks.mouth,\n",
    "                relative_landmark_id=FaceLandmarks.tip_of_nose, \n",
    "                coords=['x', 'y'], \n",
    "                stats=stats)\n",
    "        output = model(data)\n",
    "        class_id = np.argmax(output)\n",
    "        class_label = viseme_config.get_class_label(class_id)\n",
    "        print(img_name, class_label, class_id, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c5e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
